### 3.6　持久化(缓存) ###
Spark RDD 是惰性求值的，而有时我们希望能多次使用同一个 RDD。  
如果简单地对 RDD 调用行动操作，Spark 每次都会重算 RDD 以及它的所有依赖。  
这在迭代算法中消耗格外大，因为迭代算法常常会多次使用同一组数据。  
```
val result = input.map(x => x*x)
println(result.count())
println(result.collect().mkString(","))
```
为了避免多次计算同一个 RDD，可以让 Spark 对数据进行持久化。  

当我们让 Spark 持久化存储一个 RDD 时，计算出 RDD 的节点会分别保存它们所求出的分区数据。  
#### 持久化级别 ####
出于不同的目的，我们可以为 RDD 选择不同的持久化级别：
-   在 Scala 和 Java 中，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。
-   在 Python 中，我们会始终序列化要持久化存储的数据，所以持久化级别默认值就是以序列化后的对象存储在 JVM 堆空间中。

当我们把数据写到磁盘或者堆外存储上时，也总是使用序列化后的数据。  
[在 Scala 中使用 persist()](S6Persist.scala)  
[在 Java 中使用 persist()](J6Persist.java)  
[在 Python 中使用 persist()](P6Persist.py)  
【注】 persist() 调用本身不会触发强制求值。
#### 从缓存中移除 ####
RDD 还有一个方法叫作 unpersist() ，调用该方法可以手动把持久化的 RDD 从缓存中移除。