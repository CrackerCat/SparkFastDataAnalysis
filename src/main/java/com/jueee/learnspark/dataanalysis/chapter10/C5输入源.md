### 10.5　输入源 ###
#### 核心数据源 ####
所有用来从核心数据源创建 DStream 的方法都位于 StreamingContext 中。  

##### 文件流 #####
要让 Spark Streaming 来处理数据，我们需要为目录名字提供统一的日期格式，文件也必须原子化创建（比如把文件移入 Spark 监控的目录）。  

用 Scala 读取目录中的文本文件流：
<pre>val logData = ssc.textFileStream(logDirectory)</pre>
用 Java 读取目录中的文本文件流：
<pre>JavaDStream<String> logData = jssc.textFileStream(logsDirectory);</pre>

##### Akka actor流 #####
要创建出一个 actor 流，需要创建一个 Akka actor，然后实现 org.apache.spark.streaming.receiver.ActorHelper 接口。  

要把输入数据从 actor 复制到 Spark Streaming 中，需要在收到新数据时调用 actor 的 store() 函数。  

#### 附加数据源 ####
可以用附加数据源接收器来从一些知名数据获取系统中接收的数据，这些接收器都作为 Spark Streaming 的组件进行独立打包了。  

它们仍然是 Spark 的一部分，不过你需要在构建文件中添加额外的包才能使用它们。  

现有的接收器包括 Twitter、Apache Kafka、Amazon Kinesis、Apache Flume，以及 ZeroMQ。  
可以通过添加与 Spark 版本匹配的 Maven 工件 spark-streaming-[projectname]_2.10 来引入这些附加接收器。

##### Apache Kafka #####
Apache Kafka（http://kafka.apache.org/）因其速度与弹性成为了一个流行的输入源。  
在工程中需要引入 Maven 工件 spark-streaming-kafka_2.10 来使用它。  
包内提供的 KafkaUtils 对象可以在 StreamingContext 和 JavaStreamingContext 中以你的 Kafka 消息创建出 DStream。  

由于 KafkaUtils 可以订阅多个主题，因此它创建出的 DStream 由成对的主题和消息组成。  
要创建出一个流数据，需要使用 StreamingContext 实例、一个由逗号隔开的 ZooKeeper 主机列表字符串、消费者组的名字（唯一名字），以及一个从主题到针对这个主题的接收器线程数的映射表来调用 createStream() 方法。

-   [Java](J5ApacheKafka.java)
-   [Scala](S5ApacheKafka.scala)
##### Apache Flume #####
Spark 提供两个不同的接收器来使用 Apache Flume（http://flume.apache.org/）。
-   推式接收器  
该接收器以 Avro 数据池的方式工作，由 Flume 向其中推数据。  
推式接收器的方法设置起来很容易，但是它不使用事务来接收数据。  
在这种方式中，接收器以 Avro 数据池的方式工作，我们需要配置 Flume 来把数据发到 Avro 数据池。

-   拉式接收器  
该接收器可以从自定义的中间数据池中拉数据，而其他进程可以使用 Flume 把数据推进该中间数据池。  
拉式接收器设置了一个专用的 Flume 数据池供 Spark Streaming 读取，并让接收器主动从数据池中拉取数据。  
这种方式的优点在于弹性较好，Spark Streaming 通过事务从数据池中读取并复制数据。  
在收到事务完成的通知前，这些数据还保留在数据池中。


-   [Java](J5ApacheFlume.java)
-   [Scala](S5ApacheFlume.scala)
#### 多数据源与集群规模 ####
使用多个接收器对于提高聚合操作中的数据获取的吞吐量非常必要（如果只用一个接收器，可能会成为性能瓶颈）。  



