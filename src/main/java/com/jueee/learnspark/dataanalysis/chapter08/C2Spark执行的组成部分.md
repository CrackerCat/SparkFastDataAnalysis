### 8.2　Spark执行的组成部分：作业、任务和步骤 ###
使用 Spark shell 实现的简单的日志分析应用。  
输入数据是一个由不同严重等级的日志消息和一些分散的空行组成的文本文件。  

Spark 提供了 toDebugString() 方法来查看 RDD 的谱系。

#### 代码 ####
使用 Spark shell 实现的简单的日志分析应用：
-   [Python](P2Components.py)
-   [Scala](S2Components.scala)
-   [Java](J2Components.java)

#### 任务内部流程 ####
一个物理步骤会启动很多任务，每个任务都是在不同的数据分区上做同样的事情。  
任务内部的流程是一样的，如下所述。
1.  从数据存储（如果该 RDD 是一个输入 RDD）或已有 RDD（如果该步骤是基于已经缓存的数据）或数据混洗的输出中获取输入数据。
2.  执行必要的操作来计算出这些操作所代表的 RDD。例如，对输入数据执行 filter() 和 map() 函数，或者进行分组或归约操作。
3.  把输出写到一个数据混洗文件中，写入外部存储，或者是发回驱动器程序（如果最终 RDD 调用的是类似 count() 这样的行动操作）。

Spark 的大部分日志信息和工具都是以步骤、任务或数据混洗为单位的。  
理解用户代码如何编译为物理执行的内容是一个高深的话题，但对调优和调试应用有非常大的帮助。

Spark 执行时有下面所列的这些流程：
-   用户代码定义RDD的有向无环图  
RDD 上的操作会创建出新的 RDD，并引用它们的父节点，这样就创建出了一个图。

-   行动操作把有向无环图强制转译为执行计划  
当你调用 RDD 的一个行动操作时，这个 RDD 就必须被计算出来。  
这也要求计算出该RDD 的父节点。Spark 调度器提交一个作业来计算所有必要的 RDD。

-   任务于集群中调度并执行  
步骤是按顺序处理的，任务则独立地启动来计算出 RDD 的一部分。  
一旦作业的最后一个步骤结束，一个行动操作也就执行完毕了。









