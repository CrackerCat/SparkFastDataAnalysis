### 2.4　独立应用 ###
连接 Spark 的过程在各语言中并不一样。  
在 Java 和 Scala 中，只需要给你的应用添加一个对于 spark-core 工件的 Maven 依赖。
```
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>${spark.jar.version}</version>
    <scope>compile</scope>
</dependency>
```
在 Python 中，你可以把应用写成 Python 脚本，使用 Spark 自带的 bin/spark-submit 脚本来运行。   
spark-submit 脚本会帮我们引入 Python 程序的 Spark 依赖。  
```
E:\software\spark\spark-2.3.1-bin-hadoop2.7\bin>spark-submit P2PythonShell.py
18/07/23 14:36:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
sc: <SparkContext master=local appName=P2PythonShell.py>
lines: spark-sql MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0
25
#!/usr/bin/env bash
```
#### 2.4.1　初始化SparkContext ####
一旦完成了应用与 Spark 的连接，接下来就需要在你的程序中导入 Spark 包并且创建 SparkContext。  
你可以通过先创建一个 SparkConf 对象来配置你的应用，然后基于这个 SparkConf 创建一个 SparkContext 对象。  
[Java 版本](J41InitSparkContext.java)  
[Python 版本](P41InitSparkContext.py)  
[Scala 版本](S41InitSparkContext.scala)  
创建 SparkContext 的最基本的方法，只需传递两个参数：
-   集群 URL：告诉 Spark 如何连接到集群上。
-   应用名：在例子中我们使用的是 My App 。  

关闭 Spark 可以调用 SparkContext 的 stop() 方法，或者直接退出应用（比如通过 System.exit(0) 或者 sys.exit() ）。
#### 2.4.2　构建独立应用 ####
单词数统计应用：  
[Java 版本](J42WordCount.java)  
[Python 版本](P42WordCount.py)  
[Scala 版本](S42WordCount.scala)   
##### sbt 构建 #####
```sbt clean package```
##### Maven 构建 #####
```mvn clean && mvn compile && mvn package```

##### 运行 #####
```使用 bin/spark-submit 脚本执行我们的应用```
 spark-submit 脚本可以为我们配置 Spark 所要用到的一系列环境变量。