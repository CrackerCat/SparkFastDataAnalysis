### 7.4　打包代码与依赖 ###
-   可以使用 spark-submit 的 --py-Files 参数提交独立的库，这样它们也会被添加到 Python 解释器的路径中。
-   Java 和 Scala 用户也可以通过 spark-submit 的 --jars 标记提交独立的 JAR 包依赖。

#### 使用Maven构建的用Java编写的Spark应用 ####
使用 maven-shade-plugin 插件来创建出包含所有依赖的超级 JAR 包。
``` 
<!-- 用来创建超级JAR包的Maven shade插件 -->
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <version>2.3</version>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
        </execution>
    </executions>
</plugin>
```

#### 使用sbt构建的用Scala编写的Spark应用 ####
sbt 是一个通常在 Scala 工程中使用的比较新的构建工具。  
sbt 预期的目录结构和 Maven 相似。  

### 依赖冲突 ###
当用户应用与 Spark 本身依赖同一个库时可能会发生依赖冲突，导致程序崩溃:
主要有两种解决方式：
-   修改你的应用，使其使用的依赖库的版本与 Spark 所使用的相同
-   使用通常被称为“shading”的方式打包你的应用。  
Maven 构建工具通过使用插件 maven-shade-plugin 进行高级配置来支持这种打包方式。