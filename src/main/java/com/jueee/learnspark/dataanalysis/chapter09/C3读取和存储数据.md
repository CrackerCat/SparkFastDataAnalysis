### 9.3　读取和存储数据 ###
当你使用 SQL 查询这些数据源中的数据并且只用到了一部分字段时，Spark SQL 可以智能地只扫描这些用到的字段，而不是像 SparkContext.hadoopFile 中那样简单粗暴地扫描全部数据。  

#### 9.3.1　Apache Hive ####
从 Hive 中读取数据时，Spark SQL 支持任何 Hive 支持的存储格式（SerDe），包括文本文件、RCFiles、ORC、Parquet、Avro，以及 Protocol Buffer。  

#### 9.3.2　Parquet ####
Parquet（http://parquet.apache.org/）是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录。  
Parquet 格式经常在 Hadoop 生态圈中被使用，它也支持 Spark SQL 的全部数据类型。

#### 9.3.3　JSON ####
要读取 JSON 数据，只要调用 hiveCtx 中的 jsonFile() 方法即可。

printSchema() 输出的结构信息。

#### 9.3.4　基于RDD ####
使用 Row 和具名元组创建：
-   [Python](P34FromRDDs.py)
-   [Scala](S34FromRDDs.scala)
-   [Java](J34FromRDDs.java)















