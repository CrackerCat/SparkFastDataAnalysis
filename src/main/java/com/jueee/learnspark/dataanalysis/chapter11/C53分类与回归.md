### 11.5.3　分类与回归 ###
分类与回归是监督式学习的两种主要形式。  
监督式学习指算法尝试使用有标签的训练数据（也就是已知结果的数据点）根据对象的特征预测结果。  

分类和回归的区别在于预测的变量的类型：
-   在分类中，预测出的变量是离散的（也就是一个在有限集中的值，叫作类别）；  
比如，分类可能是将邮件分为垃圾邮件和非垃圾邮件，也有可能是文本所使用的语言。
-   在回归中，预测出的变量是连续的（例如根据年龄和体重预测一个人的身高）。  

分类和回归都会使用 MLlib 中的 LabeledPoint 类。这个类在 mllib.regression 包中。  
一个 LabeledPoint 其实就是由一个 label （ label 总是一个 Double 值，不过可以为分类算法设为离散整数）和一个 features 向量组成。  

### 线性回归 ###
线性回归是回归中最常用的方法之一，是指用特征的线性组合来预测输出值。  
MLlib 也支持 L1和 L2的正则的回归，通常称为 Lasso 和 ridge 回归。  

线性回归算法可以使用的类包括 mllib.regression.LinearRegressionWithSGD 、 LassoWithSGD 以及 RidgeRegressionWithSGD 。
-   这遵循了 MLlib 中常用的命名模式，即对于涉及多个算法的问题，在类名中使用“With”来表明所使用的算法。  

这里，SGD 代表的是随机梯度下降法。

这些类都有几个可以用来对算法进行调优的参数。
-   numIterations  
要运行的迭代次数（默认值： 100 ）。
-   stepSize  
梯度下降的步长（默认值： 1.0 ）。
-   intercept  
是否给数据加上一个干扰特征或者偏差特征——也就是一个值始终为 1 的特征（默认值： false ）。
-   regParam  
Lasso 和 ridge 的正规化参数（默认值： 1.0 ）。  

#### 调用算法 ####
-   在Java和Scala中， 你需要创建一个LinearRegressionWithSGD 对象，调用它的 setter 方法来设置参数，然后调用 run() 来训练模型。  
-   在 Python 中，你需要使用类的方法 LinearRegressionWithSGD.train() ，并对其传递键值对参数。  

在这两种情况中，你都需要传递一个由 LabeledPoint 组成的 RDD。  

#### 代码示例 ####
-   [Python](P53LinearRegression.py)
-   [Java](J53LinearRegression.java)  
在 Java 中，需要通过调用 .rdd() 方法把 JavaRDD 转为 Scala 中的 RDD 类。
-   [Scala](S53LinearRegression.scala)  

### 逻辑回归 ###
逻辑回归是一种二元分类方法，用来寻找一个分隔阴性和阳性示例的线性分割平面。  
在 MLlib 中，它接收一组标签为 0 或 1 的 LabeledPoint ，返回可以预测新点的分类的 LogisticRegressionModel 对象。  
#### 代码示例 ####
-   [Python](P53LogisticRegression.py)  
ImportError: cannot import name 'LogisticRegressionWithLBFGS' from 'pyspark.mllib.regression' 
-   [Java](J53LogisticRegression.java)  
-   [Scala](S53LogisticRegression.scala)  

