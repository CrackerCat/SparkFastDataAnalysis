### 11.5.1　特征提取 ###
mllib.feature 包中包含一些用来进行常见特征转化的类。  
这些类中有从文本（或其他表示）创建特征向量的算法，也有对特征向量进行正规化和伸缩变换的方法。  

### TF-IDF ###
词频—逆文档频率（简称 TF-IDF）是一种用来从文本文档（例如网页）中生成特征向量的简单方法。  

它为文档中的每个词计算两个统计值：
-   词频（TF），也就是每个词在文档中出现的次数
-   逆文档频率（IDF），用来衡量一个词在整个文档语料库中出现的（逆）频繁程度。  

这些值的积，也就是 TF × IDF，展示了一个词与特定文档的相关程度（比如这个词在某文档中很常见，但在整个语料库中却很少见）。  

MLlib 有两个算法可以用来计算 TF-IDF： HashingTF 和 IDF ，都在 mllib.feature 包内。  

#### HashingTF ####
HashingTF从一个文档中计算出给定大小的词频向量。  

为了将词与向量顺序对应起来，它使用了哈希法（hasing trick）。  
HashingTF 使用每个单词对所需向量的长度 S 取模得出的哈希值，把所有单词映射到一个 0 到 S-1 之间的数字上。  
由此我们可以保证生成一个 S 维的向量。  
在实践中，即使有多个单词被映射到同一个哈希值上，算法依然适用。  

#### IDF ####
当你构建好词频向量之后，你就可以使用 IDF 来计算逆文档频率，然后将它们与词频相乘来计算 TF-IDF。  
1.  要对 IDF 对象调用 fit() 方法来获取一个 IDFModel ，它代表语料库中的逆文档频率。
2.  对模型调用 transform() 来把 TF 向量转为 IDF 向量。

#### 代码示例 ####
-   [Python](P51FeatureExtraction.py)
-   [Java](J51FeatureExtraction.java)
-   [Scala](S51FeatureExtraction.scala)

### 缩放 ###
大多数机器学习算法都要考虑特征向量中各元素的幅值，并且在特征缩放调整为平等对待时表现得最好（例如所有的特征平均值为 0，标准差为 1）。  

当构建好特征向量之后，你可以使用 MLlib 中的 StandardScaler 类来进行这样的缩放，同时控制均值和标准差。  
需要创建一个 StandardScaler ，对数据集调用 fit() 函数来获取一个 StandardScalerModel （也就是为每一列计算平均值和标准差）  
然后使用这个模型对象的 transform() 方法来缩放一个数据集。

### 正规化 ###
在一些情况下，在准备输入数据时，把向量正规化为长度 1 也是有用的。  

使用 Normalizer 类可以实现，只要使用 Normalizer.transform(rdd) 就可以了。  

默认情况下， Normalizer 使用 L2 范式（也就是欧几里得距离），不过你可以给 Normalizer 传递一个参数 p 来使用 Lp范式。  

### Word2Vec ###
Word2Vec（https://code.google.com/p/word2vec/）是一个基于神经网络的文本特征化算法，可以用来将数据传给许多下游算法。 
Spark 在 mllib.feature.Word2Vec 类中引入了该算法的一个实现。

要训练 Word2Vec，你需要传给它一个用 String 类（每个单词用一个）的 Iterable 表示的语料库。  

Word2Vec 也推荐对单词进行正规化处理（例如全部转为小写、去除标点和数字）。  

Word2Vec 算法中模型的大小等于你的词库中的单词数乘以向量的大小（向量大小默认为 100）。  

你可能希望筛选掉不在标准字典中的单词来控制模型大小。  
一般来说，比较合适的词库大小约为 100 000 个词。













