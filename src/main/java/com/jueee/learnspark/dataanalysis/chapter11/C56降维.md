### 11.5.6　降维 ###
#### 主成分分析 ####
给定一个高维空间中的点的数据集，我们经常需要减少点的维度，来使用更简单的工具对其进行分析。  
例如，我们可能想要在二维平面上画出这些点，或者只是想减少特征的数量使得模型训练更加高效。  

机器学习社区中使用的主要的降维技术是主成分分析（简称 [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)）。  
在这种技术中，我们会把特征映射到低维空间，让数据在低维空间表示的方差最大化，从而忽略一些无用的维度。  
要计算出这种映射，我们要构建出正规化的相关矩阵，并使用这个矩阵的奇异向量和奇异值。  
与最大的一部分奇异值相对应的奇异向量可以用来重建原始数据的主要成分。

PCA 目前只在 Java 和 Scala（MLlib 1.2）中可用。  
要调用 PCA，你首先要使用 mllib.linalg.distributed.RowMatrix 类来表示你的矩阵，然后存储一个由 Vector 组成的 RDD，每行一个。  

**主成分分析** 示例代码：
-   [Java](J56PrincipalComponentAnalysis.java)  
-   [Scala](S56PrincipalComponentAnalysis.scala)

#### 奇异值分解 ####
MLlib 也提供了低层的奇异值分解（简称 SVD）原语。  

SVD 会把一个 m ×n 的矩阵 A 分解成三个矩阵 A ≈ UΣV T ，其中：
-   U 是一个正交矩阵，它的列被称为左奇异向量；
-   Σ 是一个对角线上的值均为非负数并降序排列的对角矩阵，它的对角线上的值被称为奇
异值；
-   V 是一个正交矩阵，它的列被称为右奇异向量。

对于大型矩阵，通常不需要进行完全分解，只需要分解出靠前的奇异值和与之对应的奇异向量即可。  
这样可以节省存储空间、降噪，并有利于恢复低秩矩阵。  

如果保留前 k 个奇异值，那么结果矩阵就会是 U : m ×k，Σ : k × k 以及 V :n ×k。  

要进行分解，应调用 RowMatrix 类的 computeSVD 方法。  

**奇异值分解** 示例代码：
-   [Java](J56SingularValueDecomposition.java)  
-   [Scala](S56SingularValueDecomposition.scala)